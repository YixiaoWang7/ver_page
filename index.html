<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="VER Project Page">
  <meta property="og:title" content="VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing"/>
  <meta property="og:description" content="Multi-task and Continual Learning with Mixture-of-Experts"/>
  <meta property="og:url" content="anonymous"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing">
  <meta name="twitter:description" content="Multi-task and Continual Learning with Mixture-of-Experts">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multi-task Learning, Continual Learning, Mixture-of-Experts, Robotic Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing</h1>
             <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
               <!-- <span class="author-block">
                <a href="" target="_blank">Anonymous Authors</a>
                <span class="author-block"> -->
                  <a href="https://yixiaowang7.github.io" target="_blank">Yixiao Wang</a><sup>1</sup></span>
                  <span class="author-block">
                  <a href="" target="_blank">Mingxiao Huo</a><sup>2</sup></span>
                  <span class="author-block">
                  <a href="" target="_blank">Zhixuan Liang</a><sup>3</sup></span>
                  <span class="author-block">
                  <a href="" target="_blank">Yushi Du</a><sup>4</sup></span>
                  <span class="author-block">
                  <a href="" target="_blank">Lingfeng Sun</a><sup>1</sup></span>
                  <span class="author-block">
                  <a href="" target="_blank">Haotian Lin</a><sup>2</sup></span>
                  <span class="author-block">
                  <a href="" target="_blank">Jinghuan Shang</a><sup>5</sup></span>
                  <span class="author-block">
                  <a href="" target="_blank">Chensheng Peng</a><sup>1</sup></span>
                  <span class="author-block">
                  <a href="" target="_blank">Mohit Bansal</a><sup>6</sup></span>
                  <span class="author-block">
                  <a href="" target="_blank">Mingyu Ding</a><sup>6</sup><sup>&dagger;</sup></span>
                  <span class="author-block">
                  <a href="" target="_blank">Masayoshi Tomizuka</a><sup>1</sup></span>
                  </div>
                  <div class="is-size-5 publication-authors">
                      <span class="author-block"><sup>1</sup>University of California, Berkeley</span>
                      <span class="author-block"><sup>2</sup>Carnegie Mellon University</span>
                      <span class="author-block"><sup>3</sup>University of Hong Kong</span>
                      <span class="author-block"><sup>4</sup>Peking University</span>
                      <span class="author-block"><sup>5</sup>Stony Brook University</span>
                      <span class="author-block"><sup>6</sup>The University of North Carolina at Chapel Hill</span>
                      <span class="eql-cntrb"><small><br><sup>&dagger;</sup>Indicates Corresponding Author</small></span>
                  </div> 


                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://mega.nz/folder/PZ0hgQoS#mzp0pOZlV5JrsDRnx9eeDw" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YixiaoWang7/ver" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a >
                </span>


                <!-- checkpoints link -->
                  <span class="link-block">
                    <a href="https://mega.nz/folder/DZlDhJBZ#dsp8_HLLkpL4c3FD74o5sg" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Checkpoints</span>
                  </a >
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<style>
  .video-container {
    display: flex;
    justify-content: space-around;
    align-items: center; /* Aligns videos vertically */
    flex-wrap: wrap; /* Allows elements to wrap onto the next line */
  }
  .video-box {
    text-align: center; /* Centers the caption below the video */
    margin: 10px;
  }
  .fixed_video {
    width: 480px; /* Fixed width */
    height: 360px; /* Fixed height */
    display: block; /* Ensures the video doesn't have inline whitespace */
    margin: auto; /* Centers the video within its box */
  }
  .fixed_video_wider {
    width: 720px; /* Fixed width */
    height: 360px; /* Fixed height */
    display: block; /* Ensures the video doesn't have inline whitespace */
    margin: auto; /* Centers the video within its box */
  }
  .caption {
    font-size: 16px;
    color: #333;
  }
  .hero.teaser {
    margin-bottom: 4rem; /* Increase spacing between video sections */
  }
</style>
</head>
<!-- <body>
<div class="video-container">
  <div class="video-box">
    <video class = "fixed_video" poster="" id="tree" autoplay controls muted loop >
      <source src="static/videos/pen_feature_norm.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <div class="caption"><b>Multitask - Simulation</b>.</div>
  </div>
  <div class="video-box">
    <video class = "fixed_video_wider" poster="" id="tree" autoplay controls muted loop >
      <source src="static/videos/relocate_feature_norm.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <div class="caption"><b>Sparsity of SDP</b>. During the inference, only some of the experts (in orange and pink) are activated.</div>
  </div>
</div>
</body> -->
</html>


<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Pretrained vision foundation models (VFMs) advance robotic learning via rich visual representations, yet individual VFMs typically excel only in specific domains, limiting generality across tasks. Distilling multiple VFMs into a unified representation can mitigate this limitation but often yields inflexible task-specific feature selection and requires costly full retraining to incorporate robot-domain knowledge. We propose VER, a Vision Expert transformer for Robot learning. During pretraining, VER distills multiple VFMs into a vision expert library. We then fine-tune only a lightweight routing network (fewer than 0.4% of parameters) to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. We further introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve both flexibility and precision of dynamic expert selection. Moreover, VER supports parameter-efficient finetuning for scalable expert utilization and robot-domain knowledge integration. Across 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions. Code and checkpoints are available in the supplementary materials.
            Across 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- New image section -->
<section class="hero image-section">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/structure_ver.png" alt="Description of image" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered" style="font-size: 16px;">
        Overall structure of VER. VER comprises two key components: the Base Vision Transformer (BVT), which processes images into unified representations, 
        and the Vision Expert Library (VEL), which stores a diverse set of specialized vision experts and selectively utilizes them to mimic teacher vision 
        foundation models and enhance performance in downstream robotic tasks. Our framework consists of two phases: (1) Pretraining, where we distill multiple 
        foundation models (DINOv2, ViT, CLIP) into VER; (2) Downstream Robotic Tasks, where we freeze the experts and train a lightweight 
        Robot Router (&lt;0.4% parameters) that dynamically selects task-relevant visual features to guide the policy head in generating appropriate robotic actions. 
        This two-stage approach enables efficient knowledge distillation from diverse vision foundation models and adaptive feature selection for robotic tasks.
      </h2>
    </div>
  </div>
</section>
<!-- End new image section -->
 




<!-- Section 2 -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Patch Feature Visualization</h2>
        <div class="content has-text-justified">
          <p>
            Through lightweight Robot Router training, VER learns to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. 
            Patch feature visualization shows that VER concentrates on task-relevant patches, significantly reducing extreme outliers in task-irrelevant regions (e.g., background).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Section 2 -->


<!--  Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/pen_feature_norm.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered" style="font-size: 16px;">
        <b>Patch feature visualization on pen task across 10 training random seeds.</b> 
        We find that if we only use the pretrained vision foundation model experts, they cannot focus on the task-relevant patches.
        With patch-wise routing (PER) and curriculum top-k annealing (CTA), VER can dynamically and robustly select suitable experts from the pretrained library, resulting in 
        patch features that concentrate on the task-relevant patches.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!--  Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="video-container">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/relocate_feature_norm.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered" style="font-size: 16px;">
        <b>Patch feature visualization on relocate task across 10 training random seeds.</b>
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->



<!--  Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="video-container">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/bin_cup.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered" style="font-size: 16px;">
        <b>Patch feature visualization on bin-cup task.</b> We compare Theia (left column), VER before expert selection (middle column), and VER after expert selection (right column). 
        After expert selection, VER concentrates on task-relevant objects and suppresses features from robot-related and background patches.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!--  Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="video-container">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/cross_bin.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered" style="font-size: 16px;">
        <b>Patch feature visualization on cross-bin task.</b>
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!--  Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="video-container">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/cy_plate.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered" style="font-size: 16px;">
        <b>Patch feature visualization on cylinder-plate task.</b> Most surprisingly, VER can focus on the correct patches 
        during different task stages. In the in-hand camera, when grasping the cylinder, VER focuses on the cylinder and ignores the plate. When moving to the plate, VER switches to focus on the plate.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Section 1 -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment results</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate VER on different policy heads (flow matching policy, diffusion policy, behavior cloning policy) across 17 diverse robotic tasks. Results show that across all policy heads,
            VER consistently achieves strong performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Section 1 -->

<!-- New image section -->
<section class="hero image-section">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/overall_performance.png" alt="Description of image" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered" style="font-size: 16px;">
        Performance comparison with other vision encoders on 11 tasks from Franka Kitchen, Meta-World, and Adroit environments. The same policy head from Theia is used for a fair comparison of vision encoders. 
        Our approach, VER, achieves the highest average success rate (74.7%), demonstrating its superiority.
      </h2>
    </div>
  </div>
</section>
<!-- End new image section -->

<!-- New image section -->
<section class="hero image-section">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/different_policy_head_performance.png" alt="Description of image" style="width: 100%; height: auto;">
    </div>
  </div>
</section>
<!-- End new image section -->

<!-- New image section -->
<section class="hero image-section">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/topk_tfs.png" alt="Description of image" style="width: 100%; height: auto;">
    </div>
  </div>
</section>
<!-- End new image section -->

<!-- BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>to be added</code></pre>
  </div>
</section>
<!-- End BibTex citation -->

</body>
</html>






